{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *An Introduction to*\n",
    "# Natural Language Processing\n",
    "\n",
    "#### Contributors\n",
    "Helene Willits,\n",
    "Shaina Bagri,\n",
    "Rachel Castellino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text test test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Language Syntax and Semantics\n",
    "\n",
    "There are various language semantics theories that compete in the world of Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic History of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Natural Language Processing is the process of adding structure to raw text so that it can be understood and analyzed by computers. This mainly consists of defining the properties of the text so that the computer can determine the importance of different words and the relationships between them.\n",
    "\n",
    "### How do we get Computers to Understand Words?\n",
    "Computers are not very good at understanding words. However, they are built to understand numbers. In order to create complex relationships between words, NLP represents words using number vectors that make it easy to build and analyze the relationships between words.\n",
    "\n",
    "### Preprocessing\n",
    "We begin creating our NLP model by preprocessing the data. This step helps us to remove any noise in the data and reduce the ambiguity of our resulting model. There are many ways that Engineers do this, but here are some of the most common ones:\n",
    "\n",
    "##### Splitting\n",
    "The raw text is split into sections so that important sections can be more easily identified.\n",
    "\n",
    "##### Deduplication\n",
    "Similar sections of the text are grouped to remove redundancy and start forming relationships.\n",
    "\n",
    "##### Normalization\n",
    "Data that is not standard is clarified.\n",
    "\n",
    "##### Stratification\n",
    "Important parts of the text are selected to be focused on when performing the upcoming analysis.\n",
    "\n",
    "### Parsing\n",
    "The next step in our language processing is to parse the data. This means separating the data into various elements such as words, punctuation, phrases, and more.\n",
    "\n",
    "##### Tokenization\n",
    "Words and sentences are identified so that they can be processed.\n",
    "\n",
    "##### Part-of-Speech Tagging\n",
    "Words are classified based on their grammatical rules, giving them tags as Nouns, Verbs, Adjectives, and more.\n",
    "\n",
    "##### Stemming / Lemmatization\n",
    "Words are connected back to their root, so that verbs like *wanting*, *wanted*, and *wants* are all mapped back to the root *want*.\n",
    "\n",
    "##### Dependency Parsing\n",
    "Relationships between words within a sentence are identified. This is done by applying deep learning algorithms. For example, in the sentence \"I love my fluffy dog,\" Dependency Parsing would identify that the adjective \"fluffy\" is meant to describe the noun \"dog\" and that the verb \"love\" refers to the noun \"dog,\" but would know that there isn't an obvious relationship between the words \"love\" and \"fluffy.\" \n",
    "\n",
    "##### Clause Analysis\n",
    "Identifies clauses within a sentence. This process requires both supervised machine learning and linguistic rules in order to separate sentences into blocks.\n",
    "\n",
    "### Analyzing\n",
    "Now we are ready to analyze our data to find relationships between topics and trends within the data. In this step, we discover more about the text as a whole. There are various methods of analysis that are commonly used:\n",
    "\n",
    "##### Singular Value Decomposition\n",
    "The frequency of each word in the document is recorded in a matrix so that the model can determine the importance of the topics mentioned.\n",
    "\n",
    "##### Latent Dirichlet Allocation\n",
    "\n",
    "##### Categorization\n",
    "Similar pieces of text are grouped together into categories using supervised machine learning or by following a pre-made set of categorization rules.\n",
    "\n",
    "##### Sentiment\n",
    "The text is categorized by sentiment, often into buckets such as positive, negative, and neutral sentiment.\n",
    "\n",
    "##### Word Embedding\n",
    "Uses a spacial model to plot words so that words that are used in similar ways are closer together.\n",
    "\n",
    "As you can see, Natural Language Processing gives Engineers a variety of ways to define, categorize, and relate data from our text. At this point, we have created semi-structured data. Now that we know what was expressed, we can develop a model that understands the underlying meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Understanding\n",
    "Natural Language Understanding is the process that predicts the meaning of processed text. Engineers determine the intent of a message using the document's context and various methods of reducing ambiguity. This is a harder task than Natural Language Generation because of the unpredictable nature of the input text.\n",
    "\n",
    "### Extracting Information\n",
    "\n",
    "##### Noun Groups\n",
    "Identifies which noun in a sentence is the subject and uses context to determine the meaning of that subject.\n",
    "\n",
    "##### Entity Detection\n",
    "Identifies nouns as names of people, groups, places, and more in order to better understand their relationships with other words.\n",
    "\n",
    "##### Sentiment Analysis\n",
    "This version of sentiment analysis identifies the sentiment of a statement and determines its relationship with the entities in the text.\n",
    "\n",
    "##### Semantic Role Labeling\n",
    "Now that the words have been individually categorized, we can develop relationships between them with Semantic Role Labeling. For each verb, the model identifies:\n",
    "- the entity that performs the action\n",
    "- the entity that receives the action\n",
    "as well as some more roles if they apply to the verb, such as:\n",
    "- beneficiaries of the action\n",
    "This allows \n",
    "\n",
    "##### Topic Classification\n",
    "\n",
    "### Interpreting Information\n",
    "\n",
    "<!-- ##### Rule Generation\n",
    "Creates entity recognition rules using supervised machine learning.\n",
    "\n",
    "##### Text Summarization\n",
    "Identifies important information within the text and summarizes it.  -->\n",
    "\n",
    "### Graph-Based Parsing\n",
    "Graph based parsing is a traditional method of message interpretation in NLU.\n",
    "\n",
    "Using the elements of word identification that we developed so far, important concepts in the text are identified and represented as nodes. These nodes are then structured into a directed graph with edges between every node. Engineers then use algorithmic approaches to determine the relationships between nodes that represent syntactic, semantic, and topic relationships in order to predict which relationships are important.\n",
    "\n",
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Natural Language Generation (NLG)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language generation is a process that transforms structured data into human-readable English text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stages of NLG Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of ways to break down the NLG process into different stages, but the following are more common. These stages help provide a step-by-step understanding behind the concept of natural language generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, data typically contains much more information than is needed to generate the document, so it is important to establish limits for the content to determine which data is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the analyzed data needs to be interpreted and put into context. This typically happens through machine learning techniques which recognize patterns in the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is interpreted, it needs to be organized in order to create a narrative structure and a document plan. This typically results in a general document structure or template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stage is also typically referred to as microplanning. It involves choosing expressions and words within each sentence and combining sentences together based on their relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammaticalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the sentences have been clustered, the process needs to make sure that they follow correct grammar, spelling, and punctuation. Additionally, they need to follow syntax, morphology, and orthography rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the data is input into the previously generated templates and the formatting of the document is checked to make sure it is done correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Approaches to NLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two major approaches to NLG are using templates and creating documents dynamically. The following approaches show examples of these and how the approaches have built on themselves over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Gap-Filling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple gap-filling approach is one of the oldest approaches. It uses a template system in order to generate texts. This works best for texts that have a predefined structure and simply need a small amount of data to be filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scripts or Rules-Producing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach was expanded by incorporating general-purpose programming constructs through either a scripting language or business rules. The scripting approach embeds a template within a general-purpose scripting language. An example of this is using web templating languages. The business rule approaches are similar to the scripting approach but focus on writing business rules rather than scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-Level Grammatical Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts or rules-producing text approach was further developed by adding word-level grammatical functions to handle morphology, morphophonology, and orthography rules as well as their exceptions. This ensures the template systems are more complete making it easier for them to generate texts that are grammatically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Sentence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach exemplifies the transition from templates to dynamically created documents. It builds on the previous approach by using representations of the desired linguistic structure or the intended meaning to dynamically create sentences. Additionally, the system can linguistically \"optimize\" sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Document Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic document creation, then, uses the generated sentences to create a document. The document generated and the process for generating it depend on the goal of the text. For example, persuasive texts would be organized differently than informative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for Implementing NLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of models have been used to implement NLG, and the progression through them can be seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chain was one of the first algorithms used to implement NLG. It uses the current word and considers the relationships between it and every other unique word in order to predict what the next word in the sentence will be. A common example of this is when smartphones generate suggested next words while you are typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://3.bp.blogspot.com/-J3zfH_59exo/XDoZzkvKW5I/AAAAAAAAAK4/spLAxPpbY3QKexNxaEfFaJzLjxb_qwrvwCLcBGAs/s640/state.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://2.bp.blogspot.com/-U2fyhOJ7bN8/UJsL23oh3zI/AAAAAAAADRs/wZNWvVR-Jco/s1600/text-markov.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Markov Chain only uses the immediately previous word to predict the next word, RNN use all of the previous words they encounter in order to predict the next word. This memory allows the RNN to \"remember\" the background and context of the text, making them more effective to generate language. They do this by iterating through a feedforward network that calculates the probability of the next word and stores the word with the highest probability in memory. However, RNNs cannot store words encountered remotely in longer sequences and thus ends up making predictions based on only the most recent word. As such, they have difficulty generating coherent long sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Le-Lu-9/publication/313021062/figure/fig3/AS:688562659917826@1541177539464/An-example-of-a-recurrent-neural-network-language-model.ppm\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, RNN's are problematic for longer sequences. Long short-term memory addresses this weakness. This variant of RNN use a four-layer neural network consisting of the unit, the input door, the output door, and the forgotten door. These parts aid the RNN by adjusting the information flow of the unit which allows it to remember or forget words at any time interval. For example, the forgotten gate recognizes that a period can change the context of the sentence, so the current unit state information can be ignored. As a result, the network can selectively track only relevant information. However, LSTM memory is still limited due to its high complexity and thus high computational requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below image shows one unit in LSTM. Typically there are multiple units lined up that feed into each other as in the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/542/1*ULozye1lfd-dS9RSwndZdw.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is relatively new as it was first introduced in 2017. A set of encoders processes inputs with any length, and a set of decoders to return the generated sentences. As opposed to previous models, the transformer performs a small, constant number of steps while representing the words in context without needing to compress the information into a single fixed-length representation. This self-attention mechanism allows the models to maintain low computational requirements while still being able to handle longer sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/3008/1*HAArsaBKNQ0Sbof5X4e70w.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d2l.ai/_images/transformer.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it may seem like Natural Language Processing can do just about anything, NLP models are much more restricted than many people realize. NLP models are trained to perform extremely specific tasks. Even within the scope of the tasks that they are designed to perform, NLP models can have huge amounts of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Implications\n",
    "\n",
    "Even with these innate limitations, there are limits to what Natural Language Processing should be used for. NLP systems are used widely in everyday life and have an enormous effect on people's interactions with each other, technology, and the services that are available to them. Because of this, it is critical that those who are using NLP consider the implications of the work they do on the world at large and the individuals who are affected by it. \n",
    "\n",
    "Some of the most critical concerns regarding NLP at the moment are related to its ability to create or propegate discrimination against certain groups of people. With careful ethical consideration, NLP can be used in ways that minimize discrimination and increase equal access for all people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of NLP Today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/sciforce/a-comprehensive-guide-to-natural-language-generation-dd63a4b6e548\n",
    "\n",
    "https://research.aimultiple.com/nlg/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
